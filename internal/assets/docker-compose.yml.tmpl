services:
  postgres:
    image: pgvector/pgvector:pg17
    environment:
      - POSTGRES_DB=silobox
      - POSTGRES_USER=silobox
      - POSTGRES_PASSWORD=silobox
    ports:
      - "5432:5432"
    volumes:
      - {{.DataDir}}/postgres:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U silobox -d silobox"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  backend:
    image: eternis/silo-box-backend:{{.ImageTag}}
    ports:
      - "8080:8080"
    environment:
      - LLM_BASE_URL={{.LLMBaseURL}}
      - DEFAULT_MODEL={{.DefaultModel}}
      - DATABASE_URL=postgres://silobox:silobox@postgres:5432/silobox?sslmode=disable
      - PORT=8080
      - SILOD_SOCKET_PATH=/var/run/silod.sock
{{- if .EnableDeepResearch}}
      - DEEP_RESEARCH_URL=http://deep-research:{{.DeepResearchPort}}
{{- end}}
    volumes:
      - {{.SocketFile}}:/var/run/silod.sock
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      postgres:
        condition: service_healthy
{{- if .EnableDeepResearch}}
      deep-research:
        condition: service_healthy
{{- end}}
    restart: unless-stopped

  frontend:
    image: eternis/silo-box-frontend:{{.ImageTag}}
    ports:
      - "{{.Port}}:3000"
    environment:
      - NODE_ENV=production
      - PORT=3000
      - BACKEND_URL=http://backend:8080
    depends_on:
      - backend
    restart: unless-stopped
{{if .EnableInferenceEngine}}
  inference-engine:
    image: ghcr.io/ggml-org/llama.cpp:full-cuda
    ports:
      - "{{.InferencePort}}:{{.InferencePort}}"
    volumes:
      - {{.DataDir}}/models:/models:ro
    shm_size: '{{.InferenceShmSize}}'
    ipc: host
    command:
      - --server
      - -m
      - /models/{{.InferenceModelFile}}
      - -c
      - "{{.InferenceContextSize}}"
      - -b
      - "{{.InferenceBatchSize}}"
      - -ngl
      - "{{.InferenceGPULayers}}"
      - --tensor-split
      - {{.InferenceTensorSplit}}
      - -mg
      - "{{.InferenceMainGPU}}"
      - -t
      - "{{.InferenceThreads}}"
      - --threads-http
      - "{{.InferenceHTTPThreads}}"
      - -fit
      - "{{.InferenceFit}}"
      - --host
      - 0.0.0.0
      - --port
      - "{{.InferencePort}}"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [{{.InferenceGPUDevices}}]
              capabilities: [gpu]
    restart: unless-stopped
{{end}}
{{if .EnableProxyAgent}}
  silo-proxy-agent:
    image: eternis/silo-proxy-agent
    environment:
      - GRPC_SERVER_ADDRESS={{.ProxyServerURL}}
      - LOCAL_SERVICE_URL=http://frontend:3000
    depends_on:
      - frontend
    restart: unless-stopped
{{end}}
{{if .EnableDeepResearch}}
  deep-research:
    platform: linux/amd64
    image: {{.DeepResearchImage}}
    ports:
      - "{{.DeepResearchPort}}:{{.DeepResearchPort}}"
    environment:
      - LOG_LEVEL=DEBUG
      - PYTHONPATH=/usr/src
      - PYTHONUNBUFFERED=TRUE
      - MODEL_NAME={{.DefaultModel}}
      - BASE_URL={{.LLMBaseURL}}
      - API_KEY=eternisai-api-key
      - SEARCH_PROVIDER={{.SearchProvider}}
      - PERPLEXITY_API_KEY={{.PerplexityAPIKey}}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: always
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:{{.DeepResearchPort}}/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
{{end}}
