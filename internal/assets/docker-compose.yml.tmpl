services:
  postgres:
    image: pgvector/pgvector:pg17
    environment:
      - POSTGRES_DB=silobox
      - POSTGRES_USER=silobox
      - POSTGRES_PASSWORD=silobox
    ports:
      - "5432:5432"
    volumes:
      - {{.DataDir}}/postgres:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U silobox -d silobox"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  backend:
    image: eternis/silo-box-backend:{{.ImageTag}}
    ports:
      - "8080:8080"
    volumes:
      - {{.DataDir}}:/app/data
    environment:
      - LLM_BASE_URL={{.LLMBaseURL}}
      - DEFAULT_MODEL={{.DefaultModel}}
      - DATABASE_URL=postgres://silobox:silobox@postgres:5432/silobox?sslmode=disable
      - PORT=8080
      - DATA_DIR=/app/data
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      postgres:
        condition: service_healthy
      inference-engine:
        condition: service_started
    restart: unless-stopped

  frontend:
    image: eternis/silo-box-frontend:{{.ImageTag}}
    ports:
      - "{{.Port}}:3000"
    environment:
      - NODE_ENV=production
      - PORT=3000
      - BACKEND_URL=http://backend:8080
    depends_on:
      - backend
    restart: unless-stopped

  inference-engine:
    image: ghcr.io/ggml-org/llama.cpp:full-cuda
    ports:
      - "{{.InferencePort}}:{{.InferencePort}}"
    volumes:
      - {{.InferenceModelsDir}}:/models:ro
    shm_size: '{{.InferenceShmSize}}'
    ipc: host
    command:
      - --server
      - -m
      - /models/{{.InferenceModelFile}}
      - -c
      - "{{.InferenceContextSize}}"
      - -b
      - "{{.InferenceBatchSize}}"
      - -ngl
      - "{{.InferenceGPULayers}}"
      - --tensor-split
      - {{.InferenceTensorSplit}}
      - -mg
      - "{{.InferenceMainGPU}}"
      - -t
      - "{{.InferenceThreads}}"
      - --threads-http
      - "{{.InferenceHTTPThreads}}"
      - -fit
      - "{{.InferenceFit}}"
      - --host
      - 0.0.0.0
      - --port
      - "{{.InferencePort}}"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [{{.InferenceGPUDevices}}]
              capabilities: [gpu]
    restart: unless-stopped
